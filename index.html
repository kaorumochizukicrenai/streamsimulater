<!doctype html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Gemini Native Audio Dialog - Mic chunks → Chat (Single HTML)</title>
  <style>
    :root {
      --bg: #0b0f19;
      --panel: #10182a;
      --text: #e7ecff;
      --muted: #aab3d6;
      --accent: #7aa2ff;
      --bad: #ff6b6b;
      --good: #33d69f;
      --border: rgba(255,255,255,.08);
      --shadow: rgba(0,0,0,.35);
    }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Hiragino Kaku Gothic ProN", "Noto Sans JP", "Yu Gothic", sans-serif;
      background: radial-gradient(1000px 700px at 20% 0%, #12224d 0%, var(--bg) 55%);
      color: var(--text);
    }
    header {
      padding: 18px 16px;
      border-bottom: 1px solid var(--border);
      background: rgba(10,14,24,.75);
      backdrop-filter: blur(10px);
      position: sticky;
      top: 0;
      z-index: 5;
    }
    header h1 { margin: 0 0 6px; font-size: 16px; font-weight: 700; }
    header .sub { margin: 0; font-size: 12px; color: var(--muted); line-height: 1.4; }
    main {
      padding: 16px;
      display: grid;
      gap: 12px;
      grid-template-columns: 380px 1fr;
    }
    @media (max-width: 980px) { main { grid-template-columns: 1fr; } }
    .card {
      background: linear-gradient(180deg, rgba(255,255,255,.03), rgba(255,255,255,.015));
      border: 1px solid var(--border);
      border-radius: 14px;
      box-shadow: 0 10px 25px var(--shadow);
      overflow: hidden;
    }
    .card .hd {
      padding: 12px 12px 10px;
      border-bottom: 1px solid var(--border);
      background: rgba(255,255,255,.02);
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 10px;
    }
    .card .hd .title { font-size: 13px; font-weight: 700; }
    .card .bd { padding: 12px; }
    label { display: block; font-size: 12px; color: var(--muted); margin: 10px 0 6px; }
    input, textarea {
      width: 100%;
      padding: 10px 10px;
      border-radius: 10px;
      border: 1px solid var(--border);
      background: rgba(10, 14, 24, .55);
      color: var(--text);
      outline: none;
    }
    textarea { min-height: 70px; resize: vertical; }
    .row { display: grid; grid-template-columns: 1fr 1fr; gap: 10px; }
    @media (max-width: 980px) { .row { grid-template-columns: 1fr; } }
    .btns { display: flex; gap: 10px; flex-wrap: wrap; margin-top: 12px; }
    button {
      border: 1px solid var(--border);
      background: rgba(122,162,255,.15);
      color: var(--text);
      padding: 10px 12px;
      border-radius: 12px;
      cursor: pointer;
      font-weight: 700;
    }
    button:hover { background: rgba(122,162,255,.22); }
    button:disabled { cursor: not-allowed; opacity: .55; }
    .danger { background: rgba(255,107,107,.15); }
    .danger:hover { background: rgba(255,107,107,.22); }
    .ghost { background: rgba(255,255,255,.04); }
    .ghost:hover { background: rgba(255,255,255,.06); }
    .status { display: flex; align-items: center; gap: 10px; font-size: 12px; color: var(--muted); }
    .dot {
      width: 10px; height: 10px;
      border-radius: 999px;
      background: rgba(255,255,255,.18);
      box-shadow: 0 0 0 3px rgba(255,255,255,.04);
    }
    .dot.good { background: var(--good); box-shadow: 0 0 0 3px rgba(51,214,159,.15); }
    .dot.bad  { background: var(--bad);  box-shadow: 0 0 0 3px rgba(255,107,107,.15); }
    .chat {
      height: calc(100vh - 170px);
      min-height: 520px;
      overflow: auto;
      padding: 12px;
      background: linear-gradient(180deg, rgba(255,255,255,.02), rgba(255,255,255,.01));
    }
    .msg {
      max-width: 900px;
      margin: 0 0 10px;
      padding: 10px 12px;
      border-radius: 14px;
      border: 1px solid var(--border);
      background: rgba(16,24,42,.75);
      box-shadow: 0 10px 20px rgba(0,0,0,.18);
    }
    .msg.you { margin-left: auto; background: rgba(122,162,255,.12); }
    .meta { display: flex; align-items: center; gap: 8px; margin-bottom: 6px; font-size: 11px; color: var(--muted); }
    .role { font-weight: 800; color: var(--text); }
    .pill {
      font-size: 10px;
      padding: 2px 8px;
      border-radius: 999px;
      border: 1px solid var(--border);
      background: rgba(255,255,255,.04);
      color: var(--muted);
    }
    .text { white-space: pre-wrap; line-height: 1.45; font-size: 13px; }
    .small { font-size: 11px; color: var(--muted); line-height: 1.35; margin-top: 8px; }
    .warn { color: rgba(255,217,102,.95); }
  </style>
</head>
<body>
<header>
  <h1>Gemini Native Audio Dialog（音声→AI、AI→チャット表示）テスト</h1>
  <p class="sub">
    修正版：native-audio系は responseModalities=TEXT だと落ちやすいので、AUDIO + transcription で「チャット表示」を実現します。:contentReference[oaicite:2]{index=2}
  </p>
</header>

<main>
  <section class="card">
    <div class="hd">
      <div class="title">設定 / 操作</div>
      <div class="status">
        <span class="dot" id="statusDot"></span>
        <span id="statusText">未接続</span>
      </div>
    </div>
    <div class="bd">
      <label>API Key（テスト用・クライアント直置き）</label>
      <input id="apiKey" type="password" placeholder="AIza..." autocomplete="off" />

      <label>モデル名（native-audio推奨）</label>
      <input id="modelName" type="text" value="models/gemini-2.5-flash-native-audio-preview-12-2025" />

      <div class="row">
        <div>
          <label>1クール秒数（録音→送信の区切り）</label>
          <input id="cycleSec" type="number" min="1" max="15" step="1" value="3" />
        </div>
        <div>
          <label>AI音声を再生（任意）</label>
          <input id="playAudio" type="checkbox" />
          <div class="small">※OFFでもOK（出力音声は受け取りますが再生しません）</div>
        </div>
      </div>

      <label>システム指示（任意）</label>
      <textarea id="systemInstruction" placeholder="例：日本語で簡潔に返答。音声が途中で区切れていると判断したら、返答を保留して次のクールを待ってよい。"></textarea>

      <div class="btns">
        <button id="btnStart">シミュレート開始</button>
        <button id="btnStop" class="danger" disabled>シミュレート停止</button>
        <button id="btnClear" class="ghost">表示クリア</button>
      </div>

      <p class="small warn" id="note">
        ※ native-audioモデルはセッション出力が AUDIO 前提になりやすいので、チャット表示は outputAudioTranscription を使います。:contentReference[oaicite:3]{index=3}
      </p>

      <div class="small" id="debugBox"></div>
    </div>
  </section>

  <section class="card">
    <div class="hd">
      <div class="title">チャットログ</div>
      <div class="status">
        <span class="pill" id="turnInfo">turn: 0</span>
        <span class="pill" id="chunkInfo">chunk: 0</span>
      </div>
    </div>
    <div class="chat" id="chat"></div>
  </section>
</main>

<script>
/**
 * 修正版の要点:
 * - setup: responseModalities=["AUDIO"] + inputAudioTranscription + outputAudioTranscription
 *   → native-audioモデルで TEXT-only にすると 1007: Cannot extract voices... が出やすい
 *   → チャット表示は outputTranscription.text を使う
 *   参照: Live API capabilities guide. :contentReference[oaicite:4]{index=4}
 *
 * - ScriptProcessorNode は deprecated のため、AudioWorkletNode を採用（単一HTMLで Blob URL）
 */

const WS_BASE =
  "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent";

const el = (id) => document.getElementById(id);

const ui = {
  apiKey: el("apiKey"),
  modelName: el("modelName"),
  cycleSec: el("cycleSec"),
  playAudio: el("playAudio"),
  systemInstruction: el("systemInstruction"),
  btnStart: el("btnStart"),
  btnStop: el("btnStop"),
  btnClear: el("btnClear"),
  statusDot: el("statusDot"),
  statusText: el("statusText"),
  debugBox: el("debugBox"),
  chat: el("chat"),
  turnInfo: el("turnInfo"),
  chunkInfo: el("chunkInfo"),
};

let ws = null;

// Audio
let audioCtx = null;
let mediaStream = null;
let sourceNode = null;
let workletNode = null;

// output audio playback (optional)
let playbackCtx = null;

// State
let running = false;
let cycleTimer = null;
let turnCount = 0;
let chunkCount = 0;

// 1クール分のPCM(16k/16bit)を貯める
let pcmBytesQueue = []; // Array<Uint8Array>

// 表示紐付け用（入力転写が届いたら付与）
let pendingUserChunks = []; // [{id, domEl}]

// AI出力転写（turnCompleteまで蓄積）
let currentAssistantText = "";

// ---------- UI helpers ----------
function setStatus(kind, text) {
  ui.statusText.textContent = text;
  ui.statusDot.classList.remove("good", "bad");
  if (kind === "good") ui.statusDot.classList.add("good");
  if (kind === "bad") ui.statusDot.classList.add("bad");
}
function logDebug(html) { ui.debugBox.innerHTML = html; }
function updateCounters() {
  ui.turnInfo.textContent = "turn: " + turnCount;
  ui.chunkInfo.textContent = "chunk: " + chunkCount;
}
function clearChat() { ui.chat.innerHTML = ""; }
function resetStateAll() {
  turnCount = 0;
  chunkCount = 0;
  pcmBytesQueue = [];
  pendingUserChunks = [];
  currentAssistantText = "";
  updateCounters();
  setStatus("", "未接続");
  logDebug("");
}
function addMessage(role, text, extraPills=[]) {
  const msg = document.createElement("div");
  msg.className = "msg " + (role === "you" ? "you" : "ai");

  const meta = document.createElement("div");
  meta.className = "meta";

  const roleEl = document.createElement("span");
  roleEl.className = "role";
  roleEl.textContent = role === "you" ? "YOU" : "AI";
  meta.appendChild(roleEl);

  const tsEl = document.createElement("span");
  tsEl.className = "pill";
  tsEl.textContent = new Date().toLocaleTimeString();
  meta.appendChild(tsEl);

  for (const p of extraPills) {
    const pill = document.createElement("span");
    pill.className = "pill";
    pill.textContent = p;
    meta.appendChild(pill);
  }

  const body = document.createElement("div");
  body.className = "text";
  body.textContent = text;

  msg.appendChild(meta);
  msg.appendChild(body);

  ui.chat.appendChild(msg);
  ui.chat.scrollTop = ui.chat.scrollHeight;
  return msg;
}

// ---------- binary helpers ----------
function concatUint8(arrays) {
  const total = arrays.reduce((sum, a) => sum + a.length, 0);
  const out = new Uint8Array(total);
  let offset = 0;
  for (const a of arrays) { out.set(a, offset); offset += a.length; }
  return out;
}
function uint8ToBase64(u8) {
  const chunk = 0x8000;
  let binary = "";
  for (let i = 0; i < u8.length; i += chunk) {
    binary += String.fromCharCode.apply(null, u8.subarray(i, i + chunk));
  }
  return btoa(binary);
}
function base64ToUint8(b64) {
  const bin = atob(b64);
  const out = new Uint8Array(bin.length);
  for (let i = 0; i < bin.length; i++) out[i] = bin.charCodeAt(i);
  return out;
}

// ---------- audio conversion ----------
function floatToPCM16Bytes(float32) {
  const buffer = new ArrayBuffer(float32.length * 2);
  const view = new DataView(buffer);
  for (let i = 0; i < float32.length; i++) {
    let s = Math.max(-1, Math.min(1, float32[i]));
    view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7fff, true);
  }
  return new Uint8Array(buffer);
}
function downsampleTo16kAndConvertToPCM16(float32, inSampleRate) {
  const outSampleRate = 16000;
  if (inSampleRate === outSampleRate) return floatToPCM16Bytes(float32);

  const ratio = inSampleRate / outSampleRate;
  const outLength = Math.round(float32.length / ratio);
  const out = new Float32Array(outLength);

  let offsetResult = 0;
  let offsetBuffer = 0;
  while (offsetResult < out.length) {
    const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
    let accum = 0, count = 0;
    for (let i = offsetBuffer; i < nextOffsetBuffer && i < float32.length; i++) {
      accum += float32[i]; count++;
    }
    out[offsetResult] = accum / Math.max(1, count);
    offsetResult++;
    offsetBuffer = nextOffsetBuffer;
  }
  return floatToPCM16Bytes(out);
}

// ---------- WebSocket ----------
function wsSend(obj) {
  if (!ws || ws.readyState !== WebSocket.OPEN) return;
  ws.send(JSON.stringify(obj));
}

async function connectWS() {
  const key = ui.apiKey.value.trim();
  if (!key) throw new Error("API Key が空です");

  const url = WS_BASE + "?key=" + encodeURIComponent(key);
  ws = new WebSocket(url);

  ws.onopen = () => {
    setStatus("good", "接続中（setup送信）");

    const model = ui.modelName.value.trim();
    const systemInstruction = ui.systemInstruction.value.trim();

    // ★重要：native-audio系の安定運用として AUDIO モダリティで開始
    // ★チャット表示は outputAudioTranscription / inputAudioTranscription を利用
    // 公式ガイド参照 :contentReference[oaicite:5]{index=5}
    const setup = {
      setup: {
        model,
        generationConfig: {
          responseModalities: ["AUDIO"],
          temperature: 0.4,
          maxOutputTokens: 1024,
        },
        // transcription configs (camelCase)
        inputAudioTranscription: {},
        outputAudioTranscription: {},
        ...(systemInstruction ? { systemInstruction } : {}),
      }
    };
    wsSend(setup);
  };

  ws.onclose = (e) => {
    setStatus("bad", "切断: " + (e.reason || ("code=" + e.code)));
  };

  ws.onerror = () => setStatus("bad", "WebSocketエラー");

  ws.onmessage = async (ev) => {
    let msg;
    try { msg = JSON.parse(ev.data); } catch { return; }

    if (msg.setupComplete) {
      setStatus("good", "接続完了（録音中）");
      return;
    }

    if (!msg.serverContent) return;
    const sc = msg.serverContent;

    // 入力転写（ユーザー音声）
    if (sc.inputTranscription && sc.inputTranscription.text) {
      const t = sc.inputTranscription.text.trim();
      if (t && pendingUserChunks.length > 0) {
        const last = pendingUserChunks[pendingUserChunks.length - 1];
        const bodyEl = last.domEl.querySelector(".text");
        // 既に追記済みなら重複しにくいように
        if (!bodyEl.textContent.includes("[transcription]")) {
          bodyEl.textContent += "\n\n[transcription]\n" + t;
        } else {
          bodyEl.textContent += "\n" + t;
        }
      }
    }

    // 出力転写（AI音声の文字起こし → チャット表示）
    if (sc.outputTranscription && sc.outputTranscription.text) {
      currentAssistantText += sc.outputTranscription.text;
    }

    // AI音声（任意で再生）
    // modelTurn.parts[].inlineData に audio/pcm;rate=24000 が来る想定（ガイドでは出力は24kHz）:contentReference[oaicite:6]{index=6}
    if (ui.playAudio.checked && sc.modelTurn && Array.isArray(sc.modelTurn.parts)) {
      for (const part of sc.modelTurn.parts) {
        if (part.inlineData && part.inlineData.data && part.inlineData.mimeType && part.inlineData.mimeType.startsWith("audio/pcm")) {
          try { await playPcm24k(part.inlineData.data); } catch {}
        }
      }
    }

    // ターン完了で AI チャット確定
    if (sc.turnComplete) {
      const out = (currentAssistantText || "").trim();
      if (out) {
        addMessage("ai", out, ["outputTranscription", "turnComplete"]);
        turnCount++;
        updateCounters();
      }
      currentAssistantText = "";
    }
  };

  await new Promise((resolve, reject) => {
    const t = setTimeout(() => reject(new Error("WS接続タイムアウト")), 8000);
    ws.addEventListener("open", () => { clearTimeout(t); resolve(); }, { once: true });
    ws.addEventListener("error", () => { clearTimeout(t); reject(new Error("WS接続失敗")); }, { once: true });
  });
}

// ---------- AudioWorklet capture ----------
function makeCaptureWorkletURL() {
  // mono入力の Float32Array を port に流すだけの最小 worklet
  const code = `
  class CaptureProcessor extends AudioWorkletProcessor {
    process(inputs) {
      const input = inputs[0];
      if (input && input[0] && input[0].length > 0) {
        const ch0 = input[0];
        const copy = new Float32Array(ch0.length);
        copy.set(ch0);
        this.port.postMessage(copy.buffer, [copy.buffer]);
      }
      return true;
    }
  }
  registerProcessor('capture-processor', CaptureProcessor);
  `;
  const blob = new Blob([code], { type: "application/javascript" });
  return URL.createObjectURL(blob);
}

async function startMic() {
  mediaStream = await navigator.mediaDevices.getUserMedia({
    audio: {
      channelCount: 1,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
    },
    video: false
  });

  audioCtx = new (window.AudioContext || window.webkitAudioContext)();

  // AudioWorklet を追加
  const workletURL = makeCaptureWorkletURL();
  await audioCtx.audioWorklet.addModule(workletURL);
  URL.revokeObjectURL(workletURL);

  sourceNode = audioCtx.createMediaStreamSource(mediaStream);
  workletNode = new AudioWorkletNode(audioCtx, "capture-processor", { numberOfInputs: 1, numberOfOutputs: 0, channelCount: 1 });

  workletNode.port.onmessage = (ev) => {
    if (!running) return;
    const buf = ev.data; // ArrayBuffer
    const float32 = new Float32Array(buf);

    const pcm16 = downsampleTo16kAndConvertToPCM16(float32, audioCtx.sampleRate);
    pcmBytesQueue.push(pcm16);
  };

  sourceNode.connect(workletNode);
}

function stopMic() {
  try {
    if (workletNode) workletNode.disconnect();
    if (sourceNode) sourceNode.disconnect();
  } catch {}
  workletNode = null;
  sourceNode = null;

  if (mediaStream) {
    for (const tr of mediaStream.getTracks()) tr.stop();
    mediaStream = null;
  }
  if (audioCtx) {
    audioCtx.close().catch(()=>{});
    audioCtx = null;
  }
}

// ---------- chunk send loop ----------
function flushCycleSend() {
  if (!ws || ws.readyState !== WebSocket.OPEN) return;
  if (pcmBytesQueue.length === 0) return;

  const merged = concatUint8(pcmBytesQueue);
  pcmBytesQueue = [];

  const base64 = uint8ToBase64(merged);
  chunkCount++;
  updateCounters();

  // YOU message
  const domEl = addMessage(
    "you",
    `Audio chunk #${chunkCount} を送信（${merged.length} bytes / pcm16@16k）`,
    ["audio", "chunk"]
  );
  pendingUserChunks.push({ id: chunkCount, domEl });

  // クール単位で turnComplete=true → 「返答する/保留する」をモデルが選べる
  wsSend({
    clientContent: {
      turns: [
        {
          role: "user",
          parts: [
            { inlineData: { mimeType: "audio/pcm;rate=16000", data: base64 } }
          ]
        }
      ],
      turnComplete: true
    }
  });
}

// ---------- optional playback (24k PCM16 LE assumed) ----------
async function playPcm24k(base64Pcm) {
  // 出力は 24kHz PCM16LE とされる（ガイド）:contentReference[oaicite:7]{index=7}
  const bytes = base64ToUint8(base64Pcm);
  const dv = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength);
  const n = bytes.byteLength / 2;
  const float32 = new Float32Array(n);
  for (let i = 0; i < n; i++) {
    const s = dv.getInt16(i * 2, true);
    float32[i] = s / 32768;
  }

  if (!playbackCtx) playbackCtx = new (window.AudioContext || window.webkitAudioContext)();
  const buffer = playbackCtx.createBuffer(1, float32.length, 24000);
  buffer.copyToChannel(float32, 0);

  const src = playbackCtx.createBufferSource();
  src.buffer = buffer;
  src.connect(playbackCtx.destination);
  src.start();
}

// ---------- control ----------
async function startSimulation() {
  if (running) return;
  running = true;

  ui.btnStart.disabled = true;
  ui.btnStop.disabled = false;

  setStatus("", "接続準備中…");

  try {
    await connectWS();
    await startMic();

    const cycle = Math.max(1, Math.min(15, Number(ui.cycleSec.value || 3)));
    cycleTimer = setInterval(flushCycleSend, cycle * 1000);

    logDebug(`<div class="small">
      responseModalities: <b>AUDIO</b> / inputAudioTranscription: <b>ON</b> / outputAudioTranscription: <b>ON</b><br/>
      cycle: <b>${cycle}s</b>（クール単位で turnComplete=true）
    </div>`);

    addMessage("ai",
      "接続しました。話しかけてください。\n（音声が途中で区切れて不完全なら、返答を保留して次のクールを待っても構いません。）",
      ["system"]
    );

  } catch (err) {
    setStatus("bad", "開始失敗: " + err.message);
    await stopSimulation();
  }
}

async function stopSimulation() {
  running = false;

  ui.btnStart.disabled = false;
  ui.btnStop.disabled = true;

  if (cycleTimer) { clearInterval(cycleTimer); cycleTimer = null; }

  // Stop時は「忘れる」要件なので、未送信音声は破棄
  pcmBytesQueue = [];

  stopMic();

  // WSクローズ（セッション終了＝文脈破棄）
  if (ws) {
    try { ws.close(1000, "user stopped"); } catch {}
    ws = null;
  }

  // UIもリセット
  clearChat();
  resetStateAll();
}

ui.btnStart.addEventListener("click", startSimulation);
ui.btnStop.addEventListener("click", stopSimulation);
ui.btnClear.addEventListener("click", () => clearChat());

// init
resetStateAll();
</script>
</body>
</html>
